{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fauxgrad_walkthrough.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ksanjeevan/fauxgrad/blob/master/fauxgrad_walkthrough.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeyrO8krxte5"
      },
      "source": [
        "<p style=\"text-align:left;\">\n",
        "<font size=6>\n",
        "    <b>fauxgrad: simple autodiff in python</b>\n",
        "</font>\n",
        "    <span style=\"float:right;\">\n",
        "        <a href=\"https://github.com/ksanjeevan/fauxgrad\"><b>See on GitHub!</b></a>\n",
        "    </span>\n",
        "</p>\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTjQETxxxte5"
      },
      "source": [
        "Install `fauxgrad` and its dependencies by running the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goSoioHkxte5"
      },
      "source": [
        "!pip install git+https://github.com/ksanjeevan/fauxgrad.git\n",
        "!pip install ipython -U"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM9hv2SNxte6"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "* 1\\. [Implementation: the `Value` class](#val)\n",
        "\n",
        "    * 1.1\\. [Addition](#ad)\n",
        "       \n",
        "    * 1.2\\. [Multiplication](#mul)\n",
        "    \n",
        "    * 1.3\\. [Backward](#back)\n",
        "    \n",
        "    * 1.4\\. [Other functions](#func)\n",
        "    \n",
        "    \n",
        "* 2\\. [Verification: Training a Neural Net from scratch](#demo)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh3uFkNDxte6"
      },
      "source": [
        "from fauxgrad import Value\n",
        "import inspect\n",
        "from IPython import display\n",
        "\n",
        "#helper display function \n",
        "def show_code(method, cls=Value):\n",
        "    return display.Code(inspect.getsource(getattr(cls, method)), language='python')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5xEDx7nxte7"
      },
      "source": [
        "<a name=\"val\"/></a>\n",
        "# 1. Implementing the `Value` class\n",
        "\n",
        "Automatic differantiation is the technique which allows us to train deep neural networks. The idea is that whenever a function is applied to a value (*forward* pass), we should also keep track of its derivative (*backward* pass).\n",
        "\n",
        "So if we take $a = f(x)$, then we also want to compute $grad = \\partial f(x)/\\partial x$. If we have a set of operations that define a [computational](https://colah.github.io/posts/2015-08-Backprop/) [graph](https://www.geeksforgeeks.org/computational-graphs-in-deep-learning/), we'll have to keep track of these derivative values at each node. This will allow use of the chain rule to cheaply evaluate the derivative of any variable with respect to another. An example:\n",
        "\n",
        "```\n",
        "a = 5\n",
        "b = -3\n",
        "c = a * b = 5 * -3\n",
        "d = a + c = 5 + (-15)\n",
        "e = d * 2 = -10 * 2\n",
        "```\n",
        "\n",
        "We might want to know $\\partial e/ \\partial a$. Well, we can use the chain rule to figure that out:\n",
        "\n",
        "$$\\partial e/ \\partial a = \\partial e/ \\partial d * \\partial d/ \\partial a = \\partial e/ \\partial d * (\\partial a/ \\partial a + \\partial c/ \\partial a) = 2 * (1 + (-3)) = -4$$\n",
        "\n",
        "We can implement this by: \n",
        "\n",
        "* A) **Wrapping our variables** with a class that handles/implements the derivatives of the functions we wish to use. See the `__init__` of our `Value` class below where we are using:\n",
        "    \n",
        "    1. `self.parents` to keep track of the `Value`s that resulted in this one being created (if we're talking about `d` from the example above, its parents would be `[a, c]`, while `a` has parents `[]`)\n",
        "\n",
        "    2. `self.grad` to keep track of the cumulated derivative on this node\n",
        "\n",
        "    3. `self.diff` to store the partial derivatives of the function that end up acting on this `Value` (more explanation of this later)\n",
        "    \n",
        "    The code used is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apHlJztzxte7"
      },
      "source": [
        "show_code('__init__')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_kt5kzOxte8"
      },
      "source": [
        "\n",
        "* B) **Implementing each function** we want to use and its derivative (i.e. the forward and backward pass). To keep the amount of code small, let's focus on the minimum number of operations that we'll need:\n",
        "\n",
        "    - `__add__(x, y)`: adding two elements\n",
        "    - `__mul__(x, y)`: multiplying two elements\n",
        "    - `backward()`: calculating the gradient from this point backward through the computational graph\n",
        "    - `activation & loss functions`: any activation functions that we might use, or that we might use in the loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NduvA_Kxte8"
      },
      "source": [
        "<a name=\"ad\"/></a>\n",
        "## 1.1. `__add__`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMrL5ctfxte8"
      },
      "source": [
        "Our `Value` class stores the numeric data in the attribute `a.val`. So the forward pass when doing `c = a + b` will simply be `a.val + b.val` and storing it in a new `Value` (and making sure to set `c`'s parents to be `a` and `b`).\n",
        "\n",
        "Let's see the code for implementing the addition function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtmVPQm8xte9"
      },
      "source": [
        "show_code('__add__')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gmZ77X3xte9"
      },
      "source": [
        "<a name=\"mul\"/></a>\n",
        "## 1.2. `__mul__`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsGWn3nrxte9"
      },
      "source": [
        "The forward pass for the multiplication function follows the same approach as in `__add__`, but let's look at the backward pass which we ignored before.\n",
        "\n",
        "We want to take in the gradient that was propagated up to this node (formally: $\\partial J/ \\partial c$) and multiply it by our derivative value. Since we have two variables that formed `c`, our `diff` will return two values as well, corresponding to [$\\partial c/ \\partial a$ , $\\partial c/ \\partial b$] times $\\partial J/ \\partial c$. \n",
        "\n",
        "That's why in the case of `__add__` above, both derivatives are `1` and we just propagate the value of the gradient, and in `__mul__` the derivatve $\\partial c/ \\partial a = b$ and $\\partial c/ \\partial b = a$. Let's look at the `__mul__` code:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AZRBvbUxte9"
      },
      "source": [
        "show_code('__mul__')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ig2PvmqNxte-"
      },
      "source": [
        "In order for this to work fully, we'd also implement `__sub__`, `__pow__`, etc. but they all follow the same idea, and are fully done in [micrograd](https://github.com/karpathy/micrograd). Here we only implement the essential operations to later train a neural net (even though it will make some later code slightly ugly... it's a bit of a tradeoff).\n",
        "\n",
        "This does mean that this happens:\n",
        "\n",
        "```python\n",
        "from fauxgrad import Value\n",
        "Value(3) + (-2)\n",
        ">>> Value(1.00; grad=0.00)\n",
        "Value(3) - 2\n",
        ">>> Traceback (most recent call last):\n",
        ">>>   File \"<stdin>\", line 1, in <module>\n",
        ">>> TypeError: unsupported operand type(s) for -: 'Value' and 'int'\n",
        "```\n",
        "\n",
        "Which is a bit annoying but again we're not going for completeness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDA8Mjuhxte-"
      },
      "source": [
        "<a name=\"back\"/></a>\n",
        "## 1.3. `backward`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3AoNZLyxte-"
      },
      "source": [
        "This function should now backtrack from the current node back through all the parent variables, calculating their respective partial derivatives along the way. We can split this up into:\n",
        "\n",
        "* A) **Traversing the graph**. If `c` is the result of some function of `a`, we won't be able to get the gradient of `a` until we know the gradient of `c` (chain rule). So what we need is a *reverse* [topologcal ordering](https://www.geeksforgeeks.org/topological-sorting/) of the nodes in the computational graph (e.g. in the example from above we would want the ordering `[e, d, c, b, a]`).\n",
        "\n",
        "    This can be seen in the helper function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdbxLVENxte-"
      },
      "source": [
        "show_code('_rev_topo_sort')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF4uqIWPxte-"
      },
      "source": [
        "We now can take each node, and use its `diff` function to propagate the gradient to their parents. This can be seen in the `backward` function: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b0GY7lSxte_"
      },
      "source": [
        "show_code('backward')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCycP6BZxte_"
      },
      "source": [
        "We start with a `gradient=1` (since we've done the ordering correctly in `_rev_topo_sort`, `self == nodes[0]`), and for each node we update their parents' cumulated gradient. \n",
        "\n",
        "The mathematical pseudocode of what is being computed at each iteration in the outter loop is:\n",
        "\n",
        "```\n",
        "for ...:\n",
        "\n",
        "    dc/da, dc/db = c.diff(dJ/dc)\n",
        "\n",
        "    a.grad += dc/da\n",
        "    b.grad += dc/db\n",
        "```\n",
        "\n",
        "where of course a node can have two parents, no parents, one parent, etc.\n",
        "\n",
        "Let's run the example from above again using `fauxgrad` and see the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H_AIlMQvvNK"
      },
      "source": [
        "from fauxgrad import Value\n",
        "a = Value(5)\n",
        "b = Value(-3)\n",
        "c = a * b\n",
        "d = a + c\n",
        "e = d * 2\n",
        "e.backward()\n",
        "\n",
        "print(f'The derivative that we computed before, de/da = {a.grad}')\n",
        "print(f'and de/db = {b.grad}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R4BvJTyxte_"
      },
      "source": [
        "<a name=\"func\"/></a>\n",
        "## 1.4. Other Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op25AZMCxte_"
      },
      "source": [
        "We implement three more functions for our `Value` class in order to be able to run the demo in section [2.](#demo). First we can see the `relu`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJh6sYn7xte_"
      },
      "source": [
        "show_code('relu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuxD6m3hxte_"
      },
      "source": [
        "Unlike `__add__` and `__mul__`, `relu` only has one parent (the `Value` the `relu` was applied to). As we're going to solve a binary classification problem, we'll also use a `sigmoid` activation for the final layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6ArxW1gxte_"
      },
      "source": [
        "show_code('sigmoid')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpCgqAxcxte_"
      },
      "source": [
        "And we'll make of $\\sigma(x)'=\\sigma(x) * (1 - \\sigma(x))$. Finally for the implementation of our [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) loss function, we'll also need the `log`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ppf8vfnExtfA"
      },
      "source": [
        "show_code('log')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk9Yqi9extfA"
      },
      "source": [
        "Implementing all these function follows the same procedure, and extending this to `__pow__`, `__sub__` would follow the same. \n",
        "\n",
        "With a bit of helper code, we now can move on to using this minimal engine to train a neural network! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxJZ92TyMAS4"
      },
      "source": [
        "<a name=\"demo\"/></a>\n",
        "# Using `fauxgrad` to train a Neural Net for binary classification "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv36xWWbMc3f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zz5ZNXoKMjJ5"
      },
      "source": [
        "# collect 2d toy data of a donut in a donut\n",
        "from fauxgrad.utils import generate_circles, visualize_plot\n",
        "X, Y = generate_circles(num_samples=1600)\n",
        "\n",
        "# Let's display the data\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.scatter(X[:,0], X[:,1], c=np.squeeze(Y), cmap='plasma', s=100, alpha=0.7)\n",
        "ax = plt.gca()\n",
        "ax.set_facecolor((0.95, 0.95, 0.95)); plt.axis('off'); plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKHXf1pLMkoH"
      },
      "source": [
        "# Value is the engine of the automatic differentiation, while\n",
        "# Linear simply implements `List[Value]` based matrix multiplication\n",
        "# the gradients will be computed automatically!\n",
        "from fauxgrad import Value, Linear, cross_entropy\n",
        "\n",
        "class MyNet:\n",
        "    \n",
        "    def __init__(self, hidden_size=12):\n",
        "        self.l1 = Linear(2, hidden_size, 'relu')\n",
        "        self.l2 = Linear(hidden_size, 1, 'sigmoid')\n",
        "        \n",
        "    def __call__(self, x):\n",
        "        x = self.l1(x)\n",
        "        x = self.l2(x)\n",
        "        return x\n",
        "\n",
        "    def parameters(self):\n",
        "        return self.l1.parameters() + self.l2.parameters()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DbV4J3GMm08"
      },
      "source": [
        "# SGD will simply loop over the flattened parameters that were passed to it,\n",
        "# and for each one compute `p.val -= lr * p.grad`\n",
        "from fauxgrad.optim import SGD\n",
        "\n",
        "m = MyNet()\n",
        "\n",
        "data = X.reshape(100, 16, 2) \n",
        "labels = Y.reshape(100, 16)\n",
        "\n",
        "epochs = 14\n",
        "lr = 0.001\n",
        "opt = SGD(m.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    losses = []; accs = []\n",
        "    \n",
        "    for i in range(data.shape[0]):\n",
        "        x = [[Value(d[0]), Value(d[1])] for d in data[i]]\n",
        "\n",
        "        opt.zero_grad()\n",
        "        yhat = [m(datum)[0] for datum in x]\n",
        "\n",
        "        loss = cross_entropy(yhat, labels[i])\n",
        "        loss.backward()\n",
        "        \n",
        "        losses.append(loss.val)\n",
        "        accs.extend([int(y.val>0.5)==l for y, l in zip(yhat, labels[i])])\n",
        "\n",
        "        opt.step()\n",
        "    title = 'Epoch %d, Loss %.3f; Acc %.1f%%'%(epoch,\n",
        "                                               sum(losses)/len(losses), \n",
        "                                               100*sum(accs)/len(accs))\n",
        "    visualize_plot(X, Y, m, title=title)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}